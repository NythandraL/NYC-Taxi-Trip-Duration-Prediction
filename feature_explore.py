# -*- coding: utf-8 -*-
"""feature_explore.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RztaV9jJ0_oUbDVvmerXQIA-X-TPUDdq

Certain part of this code, such as distance formulas are reference from Kaggle competition.

[Code Cite](https://www.kaggle.com/karelrv/nyct-from-a-to-z-with-xgboost-tutorial)
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
import time
import numpy as np
import xgboost as xgb
from datetime import date
import holidays
import json
import math
import seaborn as sns
!pip install mapboxgl
from mapboxgl.utils import *
from mapboxgl.viz import *
# %matplotlib inline

from datetime import timedelta
import datetime as dt
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [16, 10]
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.cluster import MiniBatchKMeans
import warnings
warnings.filterwarnings('ignore')

# !pip install pyowm
# import pyowm

!pip install wwo-hist
from wwo_hist import retrieve_hist_data

!pip install -U tables
os.environ['MAPBOX_ACCESS_TOKEN'] = "pk.eyJ1IjoiY2VydWxlYW5ndSIsImEiOiJjazJ6cGs2NGUwYjhvM2JwZGVqZzl4Nmx6In0.aIKAagrGcWYMZ2x7JbDrWg"

def get_coords():
  with open('/content/drive/My Drive/10701/cx.json') as json_file:
    longitude = json.load(json_file)
    longitude['1'] = -74.17446239999998
      
  with open('/content/drive/My Drive/10701/cy.json') as json_file:
    latitude = json.load(json_file)
    latitude['1'] = 40.6895314

  return longitude, latitude

longitude, latitude = get_coords()

def haversine_distance(origin, destination):
    """
    Formula to calculate the spherical distance between 2 coordinates, with each specified as a (lat, lng) tuple

    :param origin: (lat, lng)
    :type origin: tuple
    :param destination: (lat, lng)
    :type destination: tuple
    :return: haversine distance
    :rtype: float
    """
    lat1, lon1 = origin
    lat2, lon2 = destination
    radius = 6371  # km

    dlat = math.radians(lat2 - lat1)
    dlon = math.radians(lon2 - lon1)
    a = math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat1)) * math.cos(
        math.radians(lat2)) * math.sin(dlon / 2) * math.sin(dlon / 2)
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    d = radius * c

    return d

raw_data = pd.read_hdf('/content/drive/My Drive/10701/subset.h5', 
                       parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']).sample(frac=1)
total_data = raw_data.shape[0]
split = int(total_data*0.8)
raw_data.rename(columns = {'PULocationX':'pickup_longitude', 'PULocationY':'pickup_latitude', 
                              'DOLocationX':'dropoff_longitude', 'DOLocationY':'dropoff_latitude'}, inplace = True) 
raw_train = raw_data.head(split)
raw_test = raw_data.tail(total_data - split)
print('Raw data size: ', raw_data.shape)
print('train size:', raw_train.shape)
print('test size:', raw_test.shape)

raw_train.head()

def get_pu_location(row):
  return latitude[str(row['PULocationID'])], longitude[str(row['PULocationID'])]
def get_do_location(row):
  return latitude[str(row['DOLocationID'])], longitude[str(row['DOLocationID'])]

us_holidays = holidays.UnitedStates()
def is_holiday(x):
  if x['tpep_pickup_datetime'] in us_holidays:
    return 1
  else:
    return 0

def process_test(data):
  data.drop_duplicates(inplace = True)
  data.dropna(inplace = True)

  data = data[data['PULocationID'] < 264]
  data = data[data['DOLocationID'] < 264]

  data['distance_haversine'] = data.apply(lambda x: haversine_distance((x['pickup_latitude'], 
                                                              x['pickup_longitude']), 
                                                              (x['dropoff_latitude'], 
                                                              x['dropoff_longitude'])), axis=1)
  
  # data['Month'] = data['tpep_pickup_datetime'].dt.month
  # data['DayofMonth'] = data['tpep_pickup_datetime'].dt.day
  # data['DayofWeek'] = data['tpep_pickup_datetime'].dt.dayofweek
  # data['Hour'] = data['tpep_pickup_datetime'].dt.hour
  # data['Minute'] = data['tpep_pickup_datetime'].dt.minute
  
  data['pickup_date'] = data['tpep_pickup_datetime'].dt.date
  data['dropoff_date'] = data['tpep_dropoff_datetime'].dt.date
  
  data['duration'] = (data['tpep_dropoff_datetime'] - data['tpep_pickup_datetime']).dt.total_seconds() / 60.0
  data = data[data['duration'] > 0]
  data['log_duration'] = data['duration'].map(lambda x: np.log(x) + 1)
  
  # data['isHoliday'] = data.apply(lambda x: is_holiday(x), axis=1)

  # train = pd.get_dummies(train, columns=['passenger_count', 'VendorID', 'Hour', 'Minute', 'Month', 'DayofMonth', 'DayofWeek', 'PULocationID', 'DOLocationID'])
  # ret = data.as_matrix().astype('float32')
  return data


def process_train(data):
  data = process_test(data)
    
  q = data['duration'].quantile(0.999)
  data = data[data['duration'] < q]
  # train = pd.get_dummies(train, columns=['passenger_count', 'VendorID', 'Hour', 'Minute', 'Month', 'DayofMonth', 'DayofWeek', 'PULocationID', 'DOLocationID'])
  # ret = data.as_matrix().astype('float32')
  return data

# data_df = load_csv('/content/drive/My Drive/10701/assignment1_data-1.csv')
# data_df

train = process_train(raw_train)
test = process_test(raw_test)

print('train size:', train.shape)
print('test size:', test.shape)

train.head()

plt.hist(train['duration'].values, bins=500)
plt.xlabel('duration')
plt.ylabel('number of train records')
plt.show()

plt.hist(train['log_duration'].values, bins=100)
plt.xlabel('logged duration')
plt.ylabel('number of train records')
plt.show()

plt.plot(train.groupby('PULocationID').count()[['tpep_pickup_datetime']], 'o-', label='train pickup')
plt.plot(test.groupby('PULocationID').count()[['tpep_pickup_datetime']], 'o-', label='test pickup')

# plt.plot(test.groupby('pickup_date').count()[['id']], 'o-', label='test')
plt.title('Pickup Frequency of Each Region in train and test sets')
plt.legend(loc=0)
plt.ylabel('number of records')
plt.show()

plt.plot(train.groupby('DOLocationID').count()[['tpep_dropoff_datetime']], 'o-', label='train dropoff')
plt.plot(test.groupby('DOLocationID').count()[['tpep_dropoff_datetime']], 'o-', label='test dropoff')

plt.title('Drop-off Frequency of Each Region in train and test sets')
plt.legend(loc=0)
plt.ylabel('number of records')
plt.show()

"""### let's check the train test split. It helps to decide our validation strategy and gives ideas about feature engineering"""

plt.plot(train.groupby('pickup_date').count()[['tpep_pickup_datetime']], 'o-', label='train')
plt.plot(test.groupby('pickup_date').count()[['tpep_pickup_datetime']], 'o-', label='test')
plt.title('Pickup counts by date in train and test sets')
plt.legend(loc=0)
plt.ylabel('number of records')
plt.show()

plt.plot(train.groupby('dropoff_date').count()[['tpep_dropoff_datetime']], 'o-', label='train')
plt.plot(test.groupby('dropoff_date').count()[['tpep_dropoff_datetime']], 'o-', label='test')
plt.title('Dropoff counts by date in train and test sets')
plt.legend(loc=0)
plt.ylabel('number of records')
plt.show()

city_long_border = (-74.03, -73.75)
city_lat_border = (40.63, 40.85)
N = 10000
fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True)
ax[0].scatter(train['pickup_longitude'].values, train['pickup_latitude'].values,
              color='blue', s=1, label='train', alpha=0.1)
ax[1].scatter(test['pickup_longitude'].values, test['pickup_latitude'].values,
              color='green', s=1, label='test', alpha=0.1)
fig.suptitle('Train and test area complete overlap.')
ax[0].legend(loc=0)
ax[0].set_ylabel('latitude')
ax[0].set_xlabel('longitude')
ax[1].set_xlabel('longitude')
ax[1].legend(loc=0)
plt.ylim(city_lat_border)
plt.xlim(city_long_border)
plt.show()

"""# correaltion"""

def plot_corr(col, label, title):
  plt.scatter(col, train['duration'],label=label)

  plt.title(title)
  plt.legend(loc=0)
  plt.ylabel('duration')
  plt.show()

plot_corr(train['passenger_count'], 'num passenger', 'num passenger and duration - train')

train_sub = train[train['duration'] < 200]
plt.scatter(train_sub['passenger_count'], train_sub['duration'],label='passenger_count', s=1)

plt.title('Corrplation between Passenger count and Duration - train set')
plt.legend(loc=0)
plt.ylabel('duration')
plt.xlabel('Passenger Count')
plt.show()

plot_corr(train['rain'], 'rain amount', 'rain and duration - train')

train_sub = train[train['duration'] < 200]
plt.scatter(train_sub['rain'], train_sub['duration'],label='rain amount', s=1)

plt.title('Rain amount and duration in train set')
plt.legend(loc=0)
plt.ylabel('duration')
plt.xlabel('rain (mm)')
plt.show()

plot_corr(train['snow'], 'snow amount', 'snow and duration - train')

train_sub = train[train['duration'] < 200]
plt.scatter(train_sub['snow'], train_sub['duration'],label='snow amount', s=1)

plt.title('Snow amount and duration - train')
plt.legend(loc=0)
plt.ylabel('duration')
plt.xlabel('Snow (mm)')
plt.show()

train_sub = train[train['duration'] < 200]
plt.scatter(train_sub['distance_haversine'], train_sub['duration'],label='distance amount', s=1)

plt.title('Haversine distance and duration - train')
plt.legend(loc=0)
plt.ylabel('duration')
plt.xlabel('haversine distance')
plt.show()

plot_corr(train['VendorID'], 'VendorID', 'VendorID and duration - train')



"""# PCA"""

coords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,
                    train[['dropoff_latitude', 'dropoff_longitude']].values,
                    test[['pickup_latitude', 'pickup_longitude']].values,
                    test[['dropoff_latitude', 'dropoff_longitude']].values))

pca = PCA().fit(coords)
train['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]
train['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1]
train['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]
train['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1]
test['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]
test['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1]
test['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]
test['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1]

fig, ax = plt.subplots(ncols=2)
ax[0].scatter(train['pickup_longitude'].values[:N], train['pickup_latitude'].values[:N],
              color='blue', s=1, alpha=0.1)
ax[1].scatter(train['pickup_pca0'].values[:N], train['pickup_pca1'].values[:N],
              color='green', s=1, alpha=0.1)
fig.suptitle('Pickup lat long coords and PCA transformed coords.')
ax[0].set_ylabel('latitude')
ax[0].set_xlabel('longitude')
ax[1].set_xlabel('pca0')
ax[1].set_ylabel('pca1')
ax[0].set_xlim(city_long_border)
ax[0].set_ylim(city_lat_border)
pca_borders = pca.transform([[x, y] for x in city_lat_border for y in city_long_border])
ax[1].set_xlim(pca_borders[:, 0].min(), pca_borders[:, 0].max())
ax[1].set_ylim(pca_borders[:, 1].min(), pca_borders[:, 1].max())
plt.show()

"""# distance"""

def haversine_array(lat1, lng1, lat2, lng2):
    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))
    AVG_EARTH_RADIUS = 6371  # in km
    lat = lat2 - lat1
    lng = lng2 - lng1
    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2
    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))
    return h

def dummy_manhattan_distance(lat1, lng1, lat2, lng2):
    a = haversine_array(lat1, lng1, lat1, lng2)
    b = haversine_array(lat1, lng1, lat2, lng1)
    return a + b

def bearing_array(lat1, lng1, lat2, lng2):
    AVG_EARTH_RADIUS = 6371  # in km
    lng_delta_rad = np.radians(lng2 - lng1)
    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))
    y = np.sin(lng_delta_rad) * np.cos(lat2)
    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)
    return np.degrees(np.arctan2(y, x))

# train.loc[:, 'distance_haversine'] = haversine_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)
train.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)
train.loc[:, 'direction'] = bearing_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)
train.loc[:, 'pca_manhattan'] = np.abs(train['dropoff_pca1'] - train['pickup_pca1']) + np.abs(train['dropoff_pca0'] - train['pickup_pca0'])

# test.loc[:, 'distance_haversine'] = haversine_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)
test.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)
test.loc[:, 'direction'] = bearing_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)
test.loc[:, 'pca_manhattan'] = np.abs(test['dropoff_pca1'] - test['pickup_pca1']) + np.abs(test['dropoff_pca0'] - test['pickup_pca0'])

train.loc[:, 'center_latitude'] = (train['pickup_latitude'].values + train['dropoff_latitude'].values) / 2
train.loc[:, 'center_longitude'] = (train['pickup_longitude'].values + train['dropoff_longitude'].values) / 2
test.loc[:, 'center_latitude'] = (test['pickup_latitude'].values + test['dropoff_latitude'].values) / 2
test.loc[:, 'center_longitude'] = (test['pickup_longitude'].values + test['dropoff_longitude'].values) / 2

"""# time"""

train.loc[:, 'pickup_weekday'] = train['tpep_pickup_datetime'].dt.weekday
train.loc[:, 'pickup_hour_weekofyear'] = train['tpep_pickup_datetime'].dt.weekofyear
train.loc[:, 'pickup_hour'] = train['tpep_pickup_datetime'].dt.hour
train.loc[:, 'pickup_minute'] = train['tpep_pickup_datetime'].dt.minute
train.loc[:, 'pickup_dt'] = (train['tpep_pickup_datetime'] - train['tpep_pickup_datetime'].min()).dt.total_seconds()
train.loc[:, 'pickup_week_hour'] = train['pickup_weekday'] * 24 + train['pickup_hour']

test.loc[:, 'pickup_weekday'] = test['tpep_pickup_datetime'].dt.weekday
test.loc[:, 'pickup_hour_weekofyear'] = test['tpep_pickup_datetime'].dt.weekofyear
test.loc[:, 'pickup_hour'] = test['tpep_pickup_datetime'].dt.hour
test.loc[:, 'pickup_minute'] = test['tpep_pickup_datetime'].dt.minute
test.loc[:, 'pickup_dt'] = (test['tpep_pickup_datetime'] - train['tpep_pickup_datetime'].min()).dt.total_seconds()
test.loc[:, 'pickup_week_hour'] = test['pickup_weekday'] * 24 + test['pickup_hour']

X['pickup_dt']

"""# speed"""

train.columns

train.loc[:, 'avg_speed_h'] = 1000 * train['distance_haversine'] / train['duration']
train.loc[:, 'avg_speed_m'] = 1000 * train['distance_dummy_manhattan'] / train['duration']
fig, ax = plt.subplots(ncols=3, sharey=True)
ax[0].plot(train.groupby('pickup_hour').mean()['avg_speed_h'], 'bo-', lw=2, alpha=0.7)
ax[1].plot(train.groupby('pickup_weekday').mean()['avg_speed_h'], 'go-', lw=2, alpha=0.7)
ax[2].plot(train.groupby('pickup_week_hour').mean()['avg_speed_h'], 'ro-', lw=2, alpha=0.7)
ax[0].set_xlabel('hour')
ax[1].set_xlabel('weekday')
ax[2].set_xlabel('weekhour')
ax[0].set_ylabel('average speed')
fig.suptitle('Rush hour average traffic speed')
plt.show()

t0 = dt.datetime.now()
sample_ind = np.random.permutation(len(coords))[:500000]
kmeans = MiniBatchKMeans(n_clusters=20, batch_size=10000).fit(coords[sample_ind])

train.loc[:, 'pickup_cluster'] = kmeans.predict(train[['pickup_latitude', 'pickup_longitude']])
train.loc[:, 'dropoff_cluster'] = kmeans.predict(train[['dropoff_latitude', 'dropoff_longitude']])
test.loc[:, 'pickup_cluster'] = kmeans.predict(test[['pickup_latitude', 'pickup_longitude']])
test.loc[:, 'dropoff_cluster'] = kmeans.predict(test[['dropoff_latitude', 'dropoff_longitude']])
t1 = dt.datetime.now()
print('Time till clustering: %i seconds' % (t1 - t0).seconds)

fig, ax = plt.subplots(ncols=1, nrows=1)
ax.scatter(train.pickup_longitude.values[:N], train.pickup_latitude.values[:N], s=10, lw=0,
           c=train.pickup_cluster[:N].values, cmap='tab20', alpha=0.2)
ax.set_xlim(city_long_border)
ax.set_ylim(city_lat_border)
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
!pip install mapboxgl
from mapboxgl.utils import *
from mapboxgl.viz import *
# %matplotlib inline

from datetime import timedelta
import datetime as dt
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [16, 10]

pickup_counts = train['PULocationID'].value_counts().to_dict()
dropoff_counts = train['DOLocationID'].value_counts().to_dict()

def freq(x, pu_flag):
  if pu_flag:
    return pickup_counts[x['PULocationID']]
  else: 
    return dropoff_counts[x['DOLocationID']]

train['pickup_loc_count'] = train.apply(lambda x: freq(x, 1), axis=1)
train['dropoff_loc_count'] = train.apply(lambda x: freq(x, 0), axis=1)

pickup_counts_test = test['PULocationID'].value_counts().to_dict()
dropoff_counts_test = test['DOLocationID'].value_counts().to_dict()

def freq_test(x, pu_flag):
  if pu_flag:
    return pickup_counts_test[x['PULocationID']]
  else: 
    return dropoff_counts_test[x['DOLocationID']]

test['pickup_loc_count'] = test.apply(lambda x: freq_test(x, 1), axis=1)
test['dropoff_loc_count'] = test.apply(lambda x: freq_test(x, 0), axis=1)

def visualize_map(data, center, zoom):
    """
    This is a sample method for you to get used to spatial data visualization using the Mapboxgl-jupyter library.
    
    Mapboxgl-jupyter is a location based data visualization library for Jupyter Notebooks.
    To better understand this, you may want to read the documentation: 
    https://mapbox-mapboxgl-jupyter.readthedocs-hosted.com/en/latest/
    
    To use the library, you need to register for a token by accessing: 
    https://account.mapbox.com/access-tokens/
    You need to create an account and login. Then you can see your access token by revisiting the above URL.
    """
    # Create the viz from the dataframe
    viz = CircleViz(data,
                    access_token = os.environ['MAPBOX_ACCESS_TOKEN'],
                    center = center,
                    zoom = zoom,
                  )
    # It could take several minutes to show the map
    print("showing map...")
    viz.show();

# set the center of the map
center_of_nyc = (-74, 40.73)

data = df_to_geojson(train.head(50000), lat='pickup_latitude', lon='pickup_longitude')

# visualize map of New York City
visualize_map(data=data, center=center_of_nyc, zoom=10)

def draw_heatmap(data, center, zoom):
    """
    Method to draw a heat map. You should use this method to identify the most popular pickup location in the southeast of NYC.

    :param geodata: name of GeoJSON file or object or JSON join-data weight_property
    :type geodata: string
    :param center: map center point
    :type center: tuple
    :param zoom: starting zoom level for map
    :type zoom: float
    """
    # set features for the heatmap
    heatmap_color_stops = create_color_stops([0.01,0.25,0.5,0.75,1], colors='RdPu')
    heatmap_radius_stops = [[10,1],[20,2]] #increase radius with zoom

    # create a heatmap
    viz = HeatmapViz(data,
                     access_token=os.environ['MAPBOX_ACCESS_TOKEN'],
                     color_stops=heatmap_color_stops,
                     radius_stops=heatmap_radius_stops,
                     height='500px',
                     opacity=0.9,
                     center=center,
                     zoom=zoom)
    print("drawing map...")
    viz.show()

draw_heatmap(data=data, center=center_of_nyc, zoom=9)

pickup_counts = train['PULocationID'].value_counts().to_dict()
dropoff_counts = train['DOLocationID'].value_counts().to_dict()

def freq(x, pu_flag):
  if pu_flag:
    return pickup_counts[x['PULocationID']]
  else: 
    return dropoff_counts[x['DOLocationID']]

train['pickup_loc_count'] = train.apply(lambda x: freq(x, 1), axis=1)
train['dropoff_loc_count'] = train.apply(lambda x: freq(x, 0), axis=1)

train_sub = train.head(100000)
pickup_counts = train_sub['PULocationID'].value_counts().to_dict()
dropoff_counts = train_sub['DOLocationID'].value_counts().to_dict()

def freq(x, pu_flag):
  if pu_flag:
    return pickup_counts[x['PULocationID']]
  else: 
    return dropoff_counts[x['DOLocationID']]

pu_freq = [] 
for p in train_sub['PULocationID']:
  pu_freq.append([p, longitude[str(p)], latitude[str(p)]])
  
do_freq = [] 
for d in train_sub['DOLocationID']:
  do_freq.append([d, longitude[str(d)], latitude[str(d)]])

# Create the pandas DataFrame 
pickup_df = pd.DataFrame(pu_freq, columns = ['PULocationID', 'longitude', 'latitude']) 
pickup_df['freq'] = pickup_df.apply(lambda x: freq(x, 1), axis=1)

dropoff_df = pd.DataFrame(do_freq, columns = ['DOLocationID', 'longitude', 'latitude'])
dropoff_df['freq'] = dropoff_df.apply(lambda x: freq(x, 0), axis=1)

# Create a geojson Feature Collection export from a Pandas dataframe
points = df_to_geojson(pickup_df, 
                       properties=['freq'],
                       lat='latitude', lon='longitude')

#Create a clustered circle map
color_stops = create_color_stops([1, 500, 1000, 1500, 2000, 2500, 3500, 4500, 6233], colors='YlOrRd')

viz = ClusteredCircleViz(points,
                         access_token=os.environ['MAPBOX_ACCESS_TOKEN'],
                         color_stops=color_stops,
                         radius_stops=[[1,5], [10, 10], [50, 15], [100, 20]],
                         radius_default=1,
                         cluster_maxzoom=20,
                         cluster_radius=20,
                         label_size=12,
                         opacity=0.5,
                         center=center_of_nyc,
                         zoom=10)

viz.show()

# Create a geojson Feature Collection export from a Pandas dataframe
points_do = df_to_geojson(dropoff_df, 
                       properties=['freq'],
                       lat='latitude', lon='longitude')

#Create a clustered circle map
color_stops_do = create_color_stops([1, 500, 1000, 1500, 2000, 2500, 3000, 3500], colors='YlOrRd')

viz = ClusteredCircleViz(points_do,
                         access_token=os.environ['MAPBOX_ACCESS_TOKEN'],
                         color_stops=color_stops_do,
                         radius_stops=[[1,5], [10, 10], [50, 15], [100, 20]],
                         radius_default=1,
                         cluster_maxzoom=20,
                         cluster_radius=20,
                         label_size=12,
                         opacity=0.5,
                         center=center_of_nyc,
                         zoom=10)

viz.show()

weather_df = pd.read_csv('/content/drive/My Drive/10701/weather.csv', parse_dates=['date_time'])

# pd.Timestamp(2017, 1, 1, 2)
def get_snow(x):
  time = x['tpep_pickup_datetime'].dt
  comp_time = pd.Timestamp(2017, time.month, time.day, time.hour)
  return weather_df[weather_df['date_time'] == comp_time]['totalSnow_cm'].iloc[0]

# pd.Timestamp(2017, 1, 1, 2)
def get_rain(x):
  time = x['tpep_pickup_datetime'].dt
  comp_time = pd.Timestamp(2017, time.month, time.day, time.hour)
  return weather_df[weather_df['date_time'] == comp_time]['precipMM'].iloc[0]

data['snow'] = data.apply(lambda x: get_snow(x), axis=1)
data['rain'] = data.apply(lambda x: get_rain(x), axis=1)

"""# xgboost"""

train.columns

# Remove fields that we do not want to train with
X = train.drop(['duration',
                'tpep_pickup_datetime' ,
                'tpep_pickup_datetime',
                'tpep_dropoff_datetime', 
                'tpep_dropoff_datetime',
                'payment_type',
                'pickup_date' ,
                'dropoff_date',
                'pickup_pca0', 
                'pickup_pca1', 
                'dropoff_pca0', 
                'dropoff_pca1',
                'avg_speed_h',
                'dropoff_cluster',
                 'pickup_cluster',
                'avg_speed_m',
                'log_duration',
                'center_latitude', 
                'center_longitude', 
                'pca_manhattan',
                 'pickup_dt'
                ], axis=1, errors='ignore')

# Extract the value you want to predict
Y = train['duration']
print('Shape of the feature matrix: {}'.format(X.shape))

X.columns

from xgboost.sklearn import XGBRegressor
# Train the model again with the entire training set
model = XGBRegressor(objective ='reg:squarederror')
model.fit(X, Y)
xgb.plot_importance(model, height=0.8)

X.columns

from sklearn.model_selection import cross_val_score
# Evaluate features with K-fold cross validation
# The higher K is, the longer it takes to run, and the higher your confidence in the score
K = 5
model = XGBRegressor(objective ='reg:squarederror')
scores = cross_val_score(model, X, Y, cv=K, scoring='neg_mean_squared_error', verbose=False)
avg_rmse = math.sqrt(abs(np.mean(scores)))

print('Average RMSE with {}-fold Cross Validation: {:.3f}'.format(K, avg_rmse))

feature_names = X.columns

X_test = test[feature_names]
y_test = test['duration']

Xtr, Xv, ytr, yv = train_test_split(X.values, Y, test_size=0.2, random_state=1987)
dtrain = xgb.DMatrix(Xtr, label=ytr)
dvalid = xgb.DMatrix(Xv, label=yv)
dtest = xgb.DMatrix(test[feature_names].values)
watchlist = [(dtrain, 'train'), (dvalid, 'valid')]

# Try different parameters! My favorite is random search :)
# xgb_pars = {'min_child_weight': 50, 'eta': 0.3, 'colsample_bytree': 0.3, 'max_depth': 10,
#             'subsample': 0.8, 'lambda': 1., 'nthread': 4, 'booster' : 'gbtree', 'silent': 1,
#             'eval_metric': 'rmse', 'objective': 'reg:linear'}

xgb_pars = {'min_child_weight': 1, 'eta': 0.5, 'colsample_bytree': 0.9, 
            'max_depth': 6,
'subsample': 0.9, 'lambda': 1., 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,
'eval_metric': 'rmse', 'objective': 'reg:linear'}
model = xgb.train(xgb_pars, dtrain, 10, watchlist, early_stopping_rounds=2,
      maximize=False, verbose_eval=1)
print('Modeling RMSLE %.5f' % model.best_score)

